{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom timm import create_model\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom tqdm import tqdm","metadata":{"_uuid":"7f7c973b-18ee-40dc-8c9b-442b7eadb4fc","_cell_guid":"789819c4-87fe-49b2-b840-7a8b28b3eb38","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:38:33.315041Z","iopub.execute_input":"2025-05-19T07:38:33.315264Z","iopub.status.idle":"2025-05-19T07:38:45.946005Z","shell.execute_reply.started":"2025-05-19T07:38:33.315234Z","shell.execute_reply":"2025-05-19T07:38:45.945245Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Dataset class for RAF-DB\nclass RAFDataset(Dataset):\n    EMOTIONS = {1: 1, 2: 5, 3: 4, 4: 0, 5: 2, 6: 3, 7: 6}  # RAF-DB labels to 0-6\n    \n    def __init__(self, root_dir, split_file, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        with open(split_file, 'r') as f:\n            lines = f.readlines()[1:]  # Skip header\n        for line in lines:\n            parts = line.strip().split(',')\n            if len(parts) != 2:\n                print(f\"Skipping malformed line: {line.strip()}\")\n                continue\n            img_name, label = parts\n            try:\n                label_int = int(label)\n                expected_subfolder = str(label_int)\n                base_img_name = img_name\n                img_path = os.path.join(root_dir, expected_subfolder, base_img_name)\n                found = False\n                if os.path.exists(img_path):\n                    found = True\n                else:\n                    if '_aligned' in base_img_name:\n                        alt_img_name = base_img_name.replace('_aligned', '')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                    elif not base_img_name.endswith('_aligned.jpg'):\n                        alt_img_name = base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                if not found:\n                    for subfolder in range(1, 8):\n                        subfolder_str = str(subfolder)\n                        img_path = os.path.join(root_dir, subfolder_str, base_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {base_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            break\n                        alt_img_name = base_img_name.replace('_aligned', '') if '_aligned' in base_img_name else base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, subfolder_str, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {alt_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            base_img_name = alt_img_name\n                            break\n                if not found:\n                    print(f\"Warning: Image not found after searching all subfolders: {base_img_name}\")\n                    continue\n                mapped_label = self.EMOTIONS[label_int]\n                self.images.append(img_path)\n                self.labels.append(mapped_label)\n            except (ValueError, KeyError):\n                print(f\"Skipping invalid label in line: {line.strip()}\")\n                continue\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.images[idx]).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"_uuid":"05574060-9f5a-47d9-8730-b88c5db46da9","_cell_guid":"f67c9f55-245c-494e-8134-d38d6e22a9eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:38:45.947689Z","iopub.execute_input":"2025-05-19T07:38:45.948081Z","iopub.status.idle":"2025-05-19T07:38:45.958106Z","shell.execute_reply.started":"2025-05-19T07:38:45.948062Z","shell.execute_reply":"2025-05-19T07:38:45.957404Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# MLCA Module (Lightweight)\nclass MLCA(nn.Module):\n    def __init__(self, x1_dim, x2_dim, embed_dim=512, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.x1_proj = nn.Linear(x1_dim, embed_dim)\n        self.x2_proj = nn.Linear(x2_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.scale = (embed_dim // num_heads) ** -0.5\n\n    def forward(self, x1, x2):\n        x1 = self.x1_proj(x1)  # (B, N, embed_dim)\n        x2 = self.x2_proj(x2)\n        B, N, C = x1.shape\n\n        q = self.q_proj(x1).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        v = self.v_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q.reshape(B * self.num_heads, N, C // self.num_heads)\n        k = k.reshape(B * self.num_heads, N, C // self.num_heads)\n        v = v.reshape(B * self.num_heads, N, C // self.num_heads)\n\n        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.bmm(attn, v)\n\n        out = out.reshape(B, self.num_heads, N, C // self.num_heads).permute(0, 2, 1, 3).reshape(B, N, C)\n        out = self.out_proj(out)\n        return out\n\n\n\n# SwinFace Model (Lightweight)\nclass SwinFace(nn.Module):\n    def __init__(self, backbone_name='swin_base_patch4_window7_224', embed_dim=512, num_heads=4, num_classes=7, max_tokens=32):\n        super().__init__()\n        self.backbone = create_model(backbone_name, pretrained=True, features_only=True)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.max_tokens = max_tokens\n\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        f1 = features[-2]  # (B, C1, H1, W1)\n        f2 = features[-1]  # (B, C2, H2, W2)\n\n        B, C1, H1, W1 = f1.shape\n        B, C2, H2, W2 = f2.shape\n\n        f1_flat = f1.flatten(2).transpose(1, 2)  # (B, H1*W1, C1)\n        f2_flat = f2.flatten(2).transpose(1, 2)  # (B, H2*W2, C2)\n\n        N = min(f1_flat.size(1), f2_flat.size(1), self.max_tokens)\n        f1_flat = f1_flat[:, :N, :]\n        f2_flat = f2_flat[:, :N, :]\n\n        if not hasattr(self, 'mlca'):\n            self.mlca = MLCA(\n                x1_dim=C1,\n                x2_dim=C2,\n                embed_dim=self.embed_dim,\n                num_heads=self.num_heads\n            ).to(x.device)\n\n        fused = self.mlca(f1_flat, f2_flat)\n        pooled = fused.mean(dim=1)\n        logits = self.classifier(pooled)\n        return logits","metadata":{"_uuid":"584aee61-2842-45e1-9ae3-823e3fbcc869","_cell_guid":"6d29da69-c5c7-492f-a6f8-096865be4094","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:38:45.958949Z","iopub.execute_input":"2025-05-19T07:38:45.959168Z","iopub.status.idle":"2025-05-19T07:38:45.979878Z","shell.execute_reply.started":"2025-05-19T07:38:45.959150Z","shell.execute_reply":"2025-05-19T07:38:45.979147Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"_uuid":"082c5167-e9e8-4d02-82b0-04d8e155eedf","_cell_guid":"32638905-23aa-4491-a8f7-fdee32ec8749","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:38:45.980496Z","iopub.execute_input":"2025-05-19T07:38:45.980658Z","iopub.status.idle":"2025-05-19T07:38:45.993106Z","shell.execute_reply.started":"2025-05-19T07:38:45.980645Z","shell.execute_reply":"2025-05-19T07:38:45.992587Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Data Loaders\ntrain_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/train',\n    split_file='/kaggle/input/train_labels.csv',\n    transform=train_transform\n)\ntest_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/test',\n    split_file='/kaggle/input/test_labels.csv',\n    transform=test_transform\n)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of test samples: {len(test_dataset)}\")\n\nif len(train_dataset) == 0 or len(test_dataset) == 0:\n    raise ValueError(\"One or both datasets are empty. Check the warnings above for missing images.\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)  # Reduced batch size to 16\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)  # Reduced batch size to 16","metadata":{"_uuid":"1e760647-29c5-445c-9657-b5dcd60b9a33","_cell_guid":"4cdd9a54-bac4-47f7-bf76-fc203b9ec465","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:38:45.993914Z","iopub.execute_input":"2025-05-19T07:38:45.994165Z","execution_failed":"2025-05-19T07:39:08.656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.amp import GradScaler\n\n# Model Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SwinFace().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\nscaler = torch.amp.GradScaler()","metadata":{"_uuid":"f4a8c67c-3da2-462f-964f-b3ee68757d61","_cell_guid":"05808d17-96d1-4c2b-af89-956376f74719","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-19T07:39:08.656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nepochs = 20\nepoch_train_losses = []\nepoch_val_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\n# Checkpointing and early stopping variables\nbest_val_acc = 0\npatience = 5\ncounter = 0\n\nfor epoch in range(epochs):\n    model.train()\n    train_losses = []\n    train_preds, train_truths = [], []\n\n    for batch_idx, (imgs, labels) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda'):  # Updated AMP syntax\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n        if torch.isnan(loss):\n            print(f\"NaN loss at epoch {epoch+1}, batch {batch_idx+1}. Skipping.\")\n            continue\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_losses.append(loss.item())\n        train_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n        train_truths.extend(labels.detach().cpu().numpy())\n\n    avg_train_loss = np.mean(train_losses)\n    train_acc = accuracy_score(train_truths, train_preds)\n    epoch_train_losses.append(avg_train_loss)\n    train_accuracies.append(train_acc)\n\n    model.eval()\n    val_losses = []\n    val_preds, val_truths = [], []\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validating\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            with torch.amp.autocast('cuda'):\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n                val_losses.append(loss.item())\n\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n    avg_val_loss = np.mean(val_losses)\n    val_acc = accuracy_score(val_truths, val_preds)\n    epoch_val_losses.append(avg_val_loss)\n    val_accuracies.append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    print(f\"Train   - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}\")\n    print(f\"Val     - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n    \n    scheduler.step()\n\n    # Checkpointing and early stopping logic\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), '/kaggle/working/best_swinface_model.pth')\n        counter = 0\n    else:\n        counter += 1\n\n    if counter >= patience:\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break","metadata":{"_uuid":"8bb0c202-1cc1-42b3-bded-a4ee45b22b81","_cell_guid":"db7685fb-4c35-49d3-9a88-b7b152a19396","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-19T07:39:08.656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Model\ntorch.save(model.state_dict(), '/kaggle/working/swinface_model.pth')","metadata":{"_uuid":"37a11110-2448-4354-8f36-0c48776673f2","_cell_guid":"7672cfde-66fa-4c26-b910-c6540e5613c0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-19T07:39:08.656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training Curves\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(epoch_train_losses)+1), epoch_train_losses, label='Train Loss')\nplt.plot(range(1, len(epoch_val_losses)+1), epoch_val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Acc')\nplt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T07:39:08.656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_swinface_model.pth'))\n\n# Evaluate best model\nmodel.eval()\nval_preds = []\nval_truths = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, desc=\"Evaluating best model\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        with torch.cuda.amp.autocast():\n            outputs = model(imgs)\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n# Confusion Matrix\ncm = confusion_matrix(val_truths, val_preds)\nemotion_labels = ['Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Neutral']\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion_labels, yticklabels=emotion_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T07:39:08.656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict Emotion Function\ndef predict_emotion(img_path, model, transform, device):\n    img = Image.open(img_path).convert('RGB')\n    img = transform(img).unsqueeze(0).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        with torch.cuda.amp.autocast():\n            output = model(img)\n            pred = output.argmax(dim=1).item()\n    \n    emotion_labels = ['Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Neutral']\n    return emotion_labels[pred]\n\n# Example usage (uncomment and replace with a valid path to test)\n# img_path = '/kaggle/input/DATASET/test/1/test_0001_aligned.jpg'\n# predicted_emotion = predict_emotion(img_path, model, test_transform, device)\n# print(f'Predicted emotion: {predicted_emotion}')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T07:39:08.656Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null}]}