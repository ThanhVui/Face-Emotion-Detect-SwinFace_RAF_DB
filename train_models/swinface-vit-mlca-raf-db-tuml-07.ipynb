{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.amp import GradScaler\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom timm import create_model\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom tqdm import tqdm\nimport pandas as pd\n","metadata":{"_uuid":"be94813e-2f7e-4dbc-aa8b-ba39611882a2","_cell_guid":"70cd91f7-6024-49f6-8c10-8e19ccdf5080","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T09:13:34.456797Z","iopub.execute_input":"2025-05-19T09:13:34.456957Z","iopub.status.idle":"2025-05-19T09:13:47.388730Z","shell.execute_reply.started":"2025-05-19T09:13:34.456943Z","shell.execute_reply":"2025-05-19T09:13:47.388187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset and processing data\n# Define labels emotions\nemotion_labels = {0: \"Happiness\", 1: \"Surprise\", 2: \"Sadness\", 3: \"Anger\", 4: \"Disgust\", 5: \"Fear\", 6: \"Neutral\"}\n\n# Dataset class for RAF-DB custom dataset\nclass RAFDataset(Dataset):\n    # Mapping lable emotion on RAF-DB\n    EMOTIONS = {1: 1, 2: 5, 3: 4, 4: 0, 5: 2, 6: 3, 7: 6}  # RAF-DB labels to 0-6\n    \n    def __init__(self, root_dir, split_file, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        with open(split_file, 'r') as f:\n            lines = f.readlines()[1:]  # Skip header\n        for line in lines:\n            parts = line.strip().split(',')\n            if len(parts) != 2:\n                print(f\"Skipping malformed line: {line.strip()}\")\n                continue\n            img_name, label = parts\n            try:\n                label_int = int(label)\n                expected_subfolder = str(label_int)\n                base_img_name = img_name\n                img_path = os.path.join(root_dir, expected_subfolder, base_img_name)\n                found = False\n                if os.path.exists(img_path):\n                    found = True\n                else:\n                    if '_aligned' in base_img_name:\n                        alt_img_name = base_img_name.replace('_aligned', '')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                    elif not base_img_name.endswith('_aligned.jpg'):\n                        alt_img_name = base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                if not found:\n                    for subfolder in range(1, 8):\n                        subfolder_str = str(subfolder)\n                        img_path = os.path.join(root_dir, subfolder_str, base_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {base_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            break\n                        alt_img_name = base_img_name.replace('_aligned', '') if '_aligned' in base_img_name else base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, subfolder_str, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {alt_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            base_img_name = alt_img_name\n                            break\n                if not found:\n                    print(f\"Warning: Image not found after searching all subfolders: {base_img_name}\")\n                    continue\n                mapped_label = self.EMOTIONS[label_int]\n                self.images.append(img_path)\n                self.labels.append(mapped_label)\n            except (ValueError, KeyError):\n                print(f\"Skipping invalid label in line: {line.strip()}\")\n                continue\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.images[idx]).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n# Data Loaders and split into train and test set data\ntrain_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/train',\n    split_file='/kaggle/input/train_labels.csv',\n    transform=train_transform\n)\ntest_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/test',\n    split_file='/kaggle/input/test_labels.csv',\n    transform=test_transform\n)\n\n# Print dataset sizes\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of test samples: {len(test_dataset)}\")\n\n# Check class distribution in the training set\ntrain_labels = [label for _, label in train_dataset]\ntrain_class_counts = pd.Series(train_labels).value_counts().sort_index()\nprint(\"\\nTraining set class distribution (Mapped labels 0-6 to emotion names):\")\nfor mapped_label, count in train_class_counts.items():\n    emotion_name = emotion_labels[mapped_label]\n    print(f\"Class {mapped_label} ({emotion_name}): {count} images\")\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n\n# Print shapes of one batch from each loader\ntrain_image, train_label = next(iter(train_loader))\nprint(f\"\\nTrain batch: Image shape {train_image.shape}, Label shape {train_label.shape}\")\n\ntest_image, test_label = next(iter(test_loader))\nprint(f\"Test batch: Image shape {test_image.shape}, Label shape {test_label.shape}\")\n\nif len(train_dataset) == 0 or len(test_dataset) == 0:\n    raise ValueError(\"One or both datasets are empty. Check the warnings above for missing images.\")","metadata":{"_uuid":"4e0ef14f-aa5a-4df3-b76f-c85d59a64ccf","_cell_guid":"e8d771cc-d21a-4279-b200-b74087a30a2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T09:47:18.792722Z","iopub.execute_input":"2025-05-19T09:47:18.793027Z","iopub.status.idle":"2025-05-19T09:47:18.890075Z","shell.execute_reply.started":"2025-05-19T09:47:18.792996Z","shell.execute_reply":"2025-05-19T09:47:18.888938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define models\n# MLCA Module (Lightweight)\nclass MLCA(nn.Module):\n    def __init__(self, x1_dim, x2_dim, embed_dim=512, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.x1_proj = nn.Linear(x1_dim, embed_dim)\n        self.x2_proj = nn.Linear(x2_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.scale = (embed_dim // num_heads) ** -0.5\n\n    def forward(self, x1, x2):\n        x1 = self.x1_proj(x1)  # (B, N, embed_dim)\n        x2 = self.x2_proj(x2)\n        B, N, C = x1.shape\n\n        q = self.q_proj(x1).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        v = self.v_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q.reshape(B * self.num_heads, N, C // self.num_heads)\n        k = k.reshape(B * self.num_heads, N, C // self.num_heads)\n        v = v.reshape(B * self.num_heads, N, C // self.num_heads)\n\n        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.bmm(attn, v)\n\n        out = out.reshape(B, self.num_heads, N, C // self.num_heads).permute(0, 2, 1, 3).reshape(B, N, C)\n        out = self.out_proj(out)\n        return out\n\n# SwinFace Model (Lightweight)\nclass SwinFace(nn.Module):\n    def __init__(self, backbone_name='swin_base_patch4_window7_224', embed_dim=512, num_heads=4, num_classes=7, max_tokens=32):\n        super().__init__()\n        self.backbone = create_model(backbone_name, pretrained=True, features_only=True)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.max_tokens = max_tokens\n\n        # C1 = 512 (features[-2]), C2 = 1024 (features[-1]) for swin_base\n        self.mlca = MLCA(x1_dim=14, x2_dim=7, embed_dim=self.embed_dim, num_heads=self.num_heads)\n\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        f1 = features[-2]  # (B, C1, H1, W1)\n        f2 = features[-1]  # (B, C2, H2, W2)\n\n        f1_flat = f1.flatten(2).transpose(1, 2)\n        f2_flat = f2.flatten(2).transpose(1, 2)\n\n        N = min(f1_flat.size(1), f2_flat.size(1), self.max_tokens)\n        f1_flat = f1_flat[:, :N, :]\n        f2_flat = f2_flat[:, :N, :]\n\n        fused = self.mlca(f1_flat, f2_flat)\n        pooled = fused.mean(dim=1)\n        logits = self.classifier(pooled)\n        return logits","metadata":{"_uuid":"867d14af-4434-4365-9465-b2df82df57fc","_cell_guid":"c2deb329-9d15-4f66-b8cc-0b6eeda15900","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T09:13:47.402119Z","iopub.execute_input":"2025-05-19T09:13:47.402353Z","iopub.status.idle":"2025-05-19T09:13:47.427771Z","shell.execute_reply.started":"2025-05-19T09:13:47.402335Z","shell.execute_reply":"2025-05-19T09:13:47.427066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SwinFace().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\nscaler = torch.amp.GradScaler()","metadata":{"_uuid":"8c24de26-3952-4b6c-a114-e9d98fe9014a","_cell_guid":"eb27323d-3d7e-475a-bab9-9035338be332","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T09:14:03.493954Z","iopub.execute_input":"2025-05-19T09:14:03.494219Z","iopub.status.idle":"2025-05-19T09:14:07.267847Z","shell.execute_reply.started":"2025-05-19T09:14:03.494200Z","shell.execute_reply":"2025-05-19T09:14:07.267322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checkpoint Management Setup\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\ncheckpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n\nepochs = 20\ntrain_losses = []\nval_losses = []\ntrain_accs = []\nval_accs = []\n\n# Early stopping variables\npatience = 5\nepochs_no_improve = 0\nbest_acc = 0.0\nstart_epoch = 0\nearly_stop = False\n\n# Load checkpoint if it exists\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    best_acc = checkpoint['best_acc']\n    train_losses = checkpoint.get('train_losses', [])\n    val_losses = checkpoint.get('val_losses', [])\n    train_accs = checkpoint.get('train_accs', [])\n    val_accs = checkpoint.get('val_accs', [])\n    print(f\"Resumed training from epoch {start_epoch} with best accuracy {best_acc*100:.2f}%\")\n\nfor epoch in range(start_epoch, epochs):\n    model.train()\n    epoch_train_losses = []\n    train_preds, train_truths = [], []\n\n    for batch_idx, (imgs, labels) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda'):\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n        if torch.isnan(loss):\n            print(f\"NaN loss at epoch {epoch+1}, batch {batch_idx+1}. Skipping.\")\n            continue\n            \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        epoch_train_losses.append(loss.item())\n        train_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n        train_truths.extend(labels.detach().cpu().numpy())\n\n    avg_train_loss = np.mean(epoch_train_losses)\n    train_acc = accuracy_score(train_truths, train_preds)\n    train_losses.append(avg_train_loss)\n    train_accs.append(train_acc)\n\n    model.eval()\n    epoch_val_losses = []\n    val_preds, val_truths = [], []\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validating\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            with torch.amp.autocast('cuda'):\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n                epoch_val_losses.append(loss.item())\n\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n    avg_val_loss = np.mean(epoch_val_losses)\n    val_acc = accuracy_score(val_truths, val_preds)\n    val_losses.append(avg_val_loss)\n    val_accs.append(val_acc)\n\n    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n    scheduler.step()\n\n    # Save checkpoint\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'best_acc': best_acc,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n    torch.save(checkpoint, checkpoint_path)\n\n    # Update best model and early stopping\n    if val_acc > best_acc:\n        best_acc = val_acc\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), \"/kaggle/working/best_model.pth\")\n        print(f\"Saved best model with Val Acc: {best_acc*100:.2f}%\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement in Val Acc for {epochs_no_improve} epochs.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            early_stop = True\n            break\n\n    if early_stop:\n        break","metadata":{"_uuid":"23736381-c420-4c47-abbe-90e921104dac","_cell_guid":"ea6fa475-3e1b-4b22-afc1-36f2784eea2b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T09:14:07.268642Z","iopub.execute_input":"2025-05-19T09:14:07.268906Z","execution_failed":"2025-05-19T09:36:32.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Model\ntorch.save(model.state_dict(), '/kaggle/working/swinface_model.pth')","metadata":{"_uuid":"8ef515f6-11d3-4477-bd8d-b5769730839d","_cell_guid":"9730457a-8ecb-419d-9b8d-7943c8cc0128","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-19T09:36:32.973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training Curves\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(epoch_train_losses)+1), epoch_train_losses, label='Train Loss')\nplt.plot(range(1, len(epoch_val_losses)+1), epoch_val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Acc')\nplt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"_uuid":"76be6b9e-dd9f-4e4c-9952-648b70565e86","_cell_guid":"0d4e4051-04d8-4651-9f58-f73c83fa6fa2","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-19T09:36:32.973Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_swinface_model.pth'))\n\n# Evaluate best model\nmodel.eval()\nval_preds = []\nval_truths = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, desc=\"Evaluating best model\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        with torch.amp.autocast('cuda'):\n            outputs = model(imgs)\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n# Confusion Matrix\ncm = confusion_matrix(val_truths, val_preds)\nemotion_labels = ['Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Neutral']\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion_labels, yticklabels=emotion_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"_uuid":"27be5341-856b-4354-8132-3cd5dd15c0ba","_cell_guid":"7128bd16-c8e6-4745-b51a-37a262a873b9","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-19T09:36:32.973Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate Model\nmodel.eval()\ncorrect = 0\ntotal = 0\nall_preds = []\nall_labels = []\n\n# Load the best model for evaluation\nif os.path.exists(\"/kaggle/working/best_model.pth\"):\n    model.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\", map_location=device))\n    print(\"Loaded best model for evaluation.\")\nelse:\n    print(\"Best model not found. Evaluating with the final model.\")\n    if os.path.exists(\"/kaggle/working/swinface_model.pth\"):\n        model.load_state_dict(torch.load(\"/kaggle/working/swinface_model.pth\", map_location=device))\n    else:\n        print(\"No saved model found for evaluation.\")\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n        images, labels = images.to(device), labels.to(device)\n        with torch.amp.autocast('cuda'):\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\nif total > 0:\n    test_acc = 100 * correct / total\n    print(f\"Test Accuracy: {test_acc:.2f}%\")\nelse:\n    print(\"Test set is empty or not loaded correctly. Cannot compute accuracy.\")\n    test_acc = 0.0","metadata":{"_uuid":"7864dc8b-d89c-4936-883c-51a6b62200e5","_cell_guid":"d09161ca-b9f0-4808-9815-3cee9ad95ca0","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-19T09:36:32.973Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict Emotion Function\ndef predict_emotion(img_path, model, transform, device):\n    img = Image.open(img_path).convert('RGB')\n    img = transform(img).unsqueeze(0).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        with torch.amp.autocast('cuda'):\n            output = model(img)\n            pred = output.argmax(dim=1).item()\n    \n    emotion_labels = ['Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Neutral']\n    return emotion_labels[pred]\n\n# Example usage\nimg_path = '/kaggle/input/raf-db-dataset/DATASET/test/4/test_0003_aligned.jpg'\npredicted_emotion = predict_emotion(img_path, model, test_transform, device)\nprint(f'Predicted emotion: {predicted_emotion}')","metadata":{"_uuid":"e74b1045-15f9-42d2-ab7c-bbffd100a274","_cell_guid":"f8c06f22-dd5d-415d-988f-1db20fdc6b5a","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-19T09:36:32.973Z"},"_kg_hide-input":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}