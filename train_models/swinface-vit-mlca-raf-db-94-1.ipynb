{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom timm import create_model\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom tqdm import tqdm","metadata":{"_uuid":"a997e6d3-06da-46dd-b44c-72738808b7f4","_cell_guid":"143f1ee0-7d02-47a3-82e7-43a38ce08a25","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:50:42.808827Z","iopub.execute_input":"2025-05-18T16:50:42.809494Z","iopub.status.idle":"2025-05-18T16:50:58.389216Z","shell.execute_reply.started":"2025-05-18T16:50:42.809467Z","shell.execute_reply":"2025-05-18T16:50:58.388644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset class for RAF-DB\nclass RAFDataset(Dataset):\n    EMOTIONS = {1: 1, 2: 5, 3: 4, 4: 0, 5: 2, 6: 3, 7: 6}  # RAF-DB labels to 0-6\n                \n    def __init__(self, root_dir, split_file, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        with open(split_file, 'r') as f:\n            lines = f.readlines()[1:]  # Skip header\n        for line in lines:\n            parts = line.strip().split(',')\n            if len(parts) != 2:\n                print(f\"Skipping malformed line: {line.strip()}\")\n                continue\n            img_name, label = parts\n            try:\n                label_int = int(label)\n                expected_subfolder = str(label_int)\n                base_img_name = img_name\n                img_path = os.path.join(root_dir, expected_subfolder, base_img_name)\n                found = False\n                if os.path.exists(img_path):\n                    found = True\n                else:\n                    if '_aligned' in base_img_name:\n                        alt_img_name = base_img_name.replace('_aligned', '')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                    elif not base_img_name.endswith('_aligned.jpg'):\n                        alt_img_name = base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                if not found:\n                    for subfolder in range(1, 8):\n                        subfolder_str = str(subfolder)\n                        img_path = os.path.join(root_dir, subfolder_str, base_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {base_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            break\n                        alt_img_name = base_img_name.replace('_aligned', '') if '_aligned' in base_img_name else base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, subfolder_str, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {alt_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            base_img_name = alt_img_name\n                            break\n                if not found:\n                    print(f\"Warning: Image not found after searching all subfolders: {base_img_name}\")\n                    continue\n                mapped_label = self.EMOTIONS[label_int]\n                self.images.append(img_path)\n                self.labels.append(mapped_label)\n            except (ValueError, KeyError):\n                print(f\"Skipping invalid label in line: {line.strip()}\")\n                continue\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.images[idx]).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"_uuid":"b3d2d93c-bfbb-4e00-b199-9ef500d00185","_cell_guid":"f29d5085-3fe0-4ad9-9aca-bebd5f10dd6c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:50:58.390251Z","iopub.execute_input":"2025-05-18T16:50:58.390668Z","iopub.status.idle":"2025-05-18T16:50:58.400642Z","shell.execute_reply.started":"2025-05-18T16:50:58.390643Z","shell.execute_reply":"2025-05-18T16:50:58.400066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MLCA Module (Lightweight)\nclass MLCA(nn.Module):\n    def __init__(self, x1_dim, x2_dim, embed_dim=512, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.x1_proj = nn.Linear(x1_dim, embed_dim)\n        self.x2_proj = nn.Linear(x2_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.scale = (embed_dim // num_heads) ** -0.5\n\n    def forward(self, x1, x2):\n        x1 = self.x1_proj(x1)  # (B, N, embed_dim)\n        x2 = self.x2_proj(x2)\n        B, N, C = x1.shape\n\n        q = self.q_proj(x1).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        v = self.v_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q.reshape(B * self.num_heads, N, C // self.num_heads)\n        k = k.reshape(B * self.num_heads, N, C // self.num_heads)\n        v = v.reshape(B * self.num_heads, N, C // self.num_heads)\n\n        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.bmm(attn, v)\n\n        out = out.reshape(B, self.num_heads, N, C // self.num_heads).permute(0, 2, 1, 3).reshape(B, N, C)\n        out = self.out_proj(out)\n        return out\n\n\n\n# SwinFace Model (Lightweight)\nclass SwinFace(nn.Module):\n    def __init__(self, backbone_name='swin_base_patch4_window7_224', embed_dim=512, num_heads=4, num_classes=7, max_tokens=32):\n        super().__init__()\n        self.backbone = create_model(backbone_name, pretrained=True, features_only=True)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.max_tokens = max_tokens\n\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        f1 = features[-2]  # (B, C1, H1, W1)\n        f2 = features[-1]  # (B, C2, H2, W2)\n\n        B, C1, H1, W1 = f1.shape\n        B, C2, H2, W2 = f2.shape\n\n        f1_flat = f1.flatten(2).transpose(1, 2)  # (B, H1*W1, C1)\n        f2_flat = f2.flatten(2).transpose(1, 2)  # (B, H2*W2, C2)\n\n        N = min(f1_flat.size(1), f2_flat.size(1), self.max_tokens)\n        f1_flat = f1_flat[:, :N, :]\n        f2_flat = f2_flat[:, :N, :]\n\n        if not hasattr(self, 'mlca'):\n            self.mlca = MLCA(\n                x1_dim=C1,\n                x2_dim=C2,\n                embed_dim=self.embed_dim,\n                num_heads=self.num_heads\n            ).to(x.device)\n\n        fused = self.mlca(f1_flat, f2_flat)\n        pooled = fused.mean(dim=1)\n        logits = self.classifier(pooled)\n        return logits\n","metadata":{"_uuid":"79343d9b-a444-4fb5-abf3-118a517d0ba8","_cell_guid":"acd5ca4a-c7f1-4050-bc20-f4a99072681f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:50:58.401519Z","iopub.execute_input":"2025-05-18T16:50:58.401736Z","iopub.status.idle":"2025-05-18T16:50:58.421149Z","shell.execute_reply.started":"2025-05-18T16:50:58.40172Z","shell.execute_reply":"2025-05-18T16:50:58.420468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"_uuid":"5e65657a-b0a5-4731-9ba7-0748d3a1987b","_cell_guid":"5d9217ea-6073-429b-bf03-ef5144c4d763","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:50:58.42274Z","iopub.execute_input":"2025-05-18T16:50:58.423168Z","iopub.status.idle":"2025-05-18T16:50:58.44031Z","shell.execute_reply.started":"2025-05-18T16:50:58.423151Z","shell.execute_reply":"2025-05-18T16:50:58.439615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Loaders\ntrain_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/train',\n    split_file='/kaggle/input/train_labels.csv',\n    transform=train_transform\n)\ntest_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/test',\n    split_file='/kaggle/input/test_labels.csv',\n    transform=test_transform\n)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of test samples: {len(test_dataset)}\")\n\nif len(train_dataset) == 0 or len(test_dataset) == 0:\n    raise ValueError(\"One or both datasets are empty. Check the warnings above for missing images.\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)  # Reduced batch size to 16\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)  # Reduced batch size to 16","metadata":{"_uuid":"bd963716-fe86-45ef-a3c7-66cf0cc555d0","_cell_guid":"de3ce13d-049e-45a4-82e8-aaec60badb6b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:50:58.441053Z","iopub.execute_input":"2025-05-18T16:50:58.441303Z","iopub.status.idle":"2025-05-18T16:51:36.903488Z","shell.execute_reply.started":"2025-05-18T16:50:58.441281Z","shell.execute_reply":"2025-05-18T16:51:36.902745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.amp import GradScaler\n\n# Model Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SwinFace().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\nscaler = torch.amp.GradScaler()","metadata":{"_uuid":"2da8c87c-2a97-47f3-b364-0b03984559db","_cell_guid":"cfecccb7-c105-48d6-9b50-0e485e5feebb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:51:36.904471Z","iopub.execute_input":"2025-05-18T16:51:36.904779Z","iopub.status.idle":"2025-05-18T16:51:40.078671Z","shell.execute_reply.started":"2025-05-18T16:51:36.904756Z","shell.execute_reply":"2025-05-18T16:51:40.078129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nepochs = 20\nepoch_train_losses = []\nepoch_val_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(epochs):\n    model.train()\n    train_losses = []\n    train_preds, train_truths = [], []\n\n    for batch_idx, (imgs, labels) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n        if torch.isnan(loss):\n            print(f\"NaN loss at epoch {epoch+1}, batch {batch_idx+1}. Skipping.\")\n            continue\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_losses.append(loss.item())\n        train_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n        train_truths.extend(labels.detach().cpu().numpy())\n\n    avg_train_loss = np.mean(train_losses)\n    train_acc = accuracy_score(train_truths, train_preds)\n    epoch_train_losses.append(avg_train_loss)\n    train_accuracies.append(train_acc)\n\n    model.eval()\n    val_losses = []\n    val_preds, val_truths = [], []\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validating\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n                val_losses.append(loss.item())\n\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n    avg_val_loss = np.mean(val_losses)\n    val_acc = accuracy_score(val_truths, val_preds)\n    epoch_val_losses.append(avg_val_loss)\n    val_accuracies.append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    print(f\"Train   - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}\")\n    print(f\"Val     - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n    \n    scheduler.step()\n","metadata":{"_uuid":"88ebcbfb-9141-4f7b-a30d-35a1044b6bad","_cell_guid":"e7848ae9-882f-4545-820a-6355cfece89b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T16:51:40.079366Z","iopub.execute_input":"2025-05-18T16:51:40.07963Z","execution_failed":"2025-05-18T16:52:54.704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Model\ntorch.save(model.state_dict(), '/kaggle/working/swinface_model.pth')","metadata":{"_uuid":"8419b768-1b37-4ec1-ae8a-b00519ab639d","_cell_guid":"2ed02f7b-ac28-4875-942d-2599ada6ccce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-18T16:52:54.705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting Functions\ndef plot_training_progress(epoch_train_losses, val_accuracies):\n    epochs = range(1, len(epoch_train_losses) + 1)\n    fig, ax1 = plt.subplots()\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Training Loss', color='tab:blue')\n    ax1.plot(epochs, epoch_train_losses, color='tab:blue', label='Training Loss')\n    ax1.tick_params(axis='y', labelcolor='tab:blue')\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Validation Accuracy', color='tab:green')\n    ax2.plot(epochs, val_accuracies, color='tab:green', label='Validation Accuracy')\n    ax2.tick_params(axis='y', labelcolor='tab:green')\n    fig.tight_layout()\n    plt.title('Training Loss and Validation Accuracy over Epochs')\n    plt.show()\n\ndef plot_confusion_matrix(truths, preds):\n    cm = confusion_matrix(truths, preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Happy', 'Surprise', 'Sad', 'Anger', 'Disgust', 'Fear', 'Neutral'],\n                yticklabels=['Happy', 'Surprise', 'Sad', 'Anger', 'Disgust', 'Fear', 'Neutral'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"_uuid":"e9679aaa-e12d-40dc-ab6d-a0aff8507e93","_cell_guid":"cdbd2808-85f2-4608-b32e-9e11ea1069ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-18T16:52:54.705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot plot_training_progresss\nplot_training_progress(epoch_train_losses, val_accuracies)\n# Plot plot_confusion_matrix\nplot_confusion_matrix(truths, preds)","metadata":{"_uuid":"f4958934-0417-4ff5-a2c0-51ea607d6bc7","_cell_guid":"a08be997-f664-42a7-871b-e2fe8fbc0c98","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-18T16:52:54.705Z"}},"outputs":[],"execution_count":null}]}