{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import all libraries\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom timm import create_model\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom tqdm import tqdm\nimport pandas as pd","metadata":{"_uuid":"5bc51dfb-1949-45a8-b269-bea897ecbd14","_cell_guid":"df3ceca1-d382-44f0-a8f1-37a83f5715bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T03:43:00.902795Z","iopub.execute_input":"2025-05-23T03:43:00.902965Z","iopub.status.idle":"2025-05-23T03:43:13.129549Z","shell.execute_reply.started":"2025-05-23T03:43:00.902950Z","shell.execute_reply":"2025-05-23T03:43:13.128811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define emotion labels based on your mapping - Load and Preprocess RAF-DB Dataset\nemotion_labels = {0: \"Happy\", 1: \"Surprise\", 2: \"Sad\", 3: \"Anger\", 4: \"Disgust\", 5: \"Fear\", 6: \"Neutral\"}\n\nclass RAFDBDataset(Dataset):\n    def __init__(self, img_dir, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        \n        # RAF-DB classes: 1=Surprise, 2=Fear, 3=Disgust, 4=Happy, 5=Sad, 6=Anger, 7=Neutral\n        # Desired mapping: Happy=0, Surprise=1, Sad=2, Anger=3, Disgust=4, Fear=5, Neutral=6\n        self.label_map = {1: 1, 2: 5, 3: 4, 4: 0, 5: 2, 6: 3, 7: 6}\n\n        # Dynamically load images and labels from folder structure\n        self.image_data = []\n        for class_folder in os.listdir(img_dir):\n            class_path = os.path.join(img_dir, class_folder)\n            if os.path.isdir(class_path) and class_folder.isdigit():\n                label = int(class_folder)  # Get RAF-DB label (1-7) from folder name\n                if label in self.label_map:\n                    for filename in os.listdir(class_path):\n                        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n                            self.image_data.append([filename, label])\n\n        if not self.image_data:\n            raise ValueError(f\"No images found in {img_dir}\")\n\n    def __len__(self):\n        return len(self.image_data)\n\n    def __getitem__(self, idx):\n        img_name, original_rafdb_label = self.image_data[idx]\n        # Map RAF-DB label (1-7) to desired label (0-6)\n        mapped_label = self.label_map[original_rafdb_label]\n        # Construct image path using the original RAF-DB label (folder name)\n        img_path = os.path.join(self.img_dir, str(original_rafdb_label), img_name)\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, mapped_label\n\n# Define enhanced transforms for training with additional augmentations\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load datasets\nbase_path = \"/kaggle/input/raf-db-dataset\"\ntrain_img_dir = os.path.join(base_path, \"DATASET/train\")\ntest_img_dir = os.path.join(base_path, \"DATASET/test\")\n\nif not os.path.exists(train_img_dir):\n    print(f\"Warning: Training image directory not found at {train_img_dir}\")\nif not os.path.exists(test_img_dir):\n    print(f\"Warning: Test image directory not found at {test_img_dir}\")\n\n# Load full training dataset with train transforms (for now)\nfull_train_dataset = RAFDBDataset(img_dir=train_img_dir, transform=train_transforms)\n\n# Split into train and validation (80% train, 20% val)\ntrain_size = int(0.8 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(\n    full_train_dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# Apply test transforms to validation set\nval_dataset.dataset.transform = test_transforms  # override transform for validation subset\n\n# Load test dataset\ntest_dataset = RAFDBDataset(img_dir=test_img_dir, transform=test_transforms)\n\n# Dataset size info\nprint(f\"Total Training Data: {len(full_train_dataset)}\")\nprint(f\"Training Split: {len(train_dataset)}\")\nprint(f\"Validation Split: {len(val_dataset)}\")\nprint(f\"Test Set: {len(test_dataset)}\")\n\n# Class distribution (based on original full training data)\ntrain_original_labels = [item[1] for item in full_train_dataset.image_data]\ntrain_class_counts = pd.Series(train_original_labels).value_counts().sort_index()\nprint(\"\\nTraining set class distribution (Original RAF-DB labels 1-7 mapped to target emotion names):\")\nfor original_label, count in train_class_counts.items():\n    mapped_label = full_train_dataset.label_map[original_label]\n    emotion_name = emotion_labels[mapped_label]\n    print(f\"Original Class {original_label} ({emotion_name}): {count} images\")\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Batch shape check\ntrain_image, train_label = next(iter(train_loader))\nprint(f\"\\nTrain batch: Image shape {train_image.shape}, Label shape {train_label.shape}\")\n\nval_image, val_label = next(iter(val_loader))\nprint(f\"Validation batch: Image shape {val_image.shape}, Label shape {val_label.shape}\")\n\ntest_image, test_label = next(iter(test_loader))\nprint(f\"Test batch: Image shape {test_image.shape}, Label shape {test_label.shape}\")","metadata":{"_uuid":"84d5eb53-a3a9-4a00-b91f-9a25f62afdeb","_cell_guid":"06f1e544-84cf-4e22-a38f-5c9010a33865","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T03:43:13.130377Z","iopub.execute_input":"2025-05-23T03:43:13.130775Z","iopub.status.idle":"2025-05-23T03:43:14.190060Z","shell.execute_reply.started":"2025-05-23T03:43:13.130756Z","shell.execute_reply":"2025-05-23T03:43:14.189403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After augmentation\n# Function to denormalize images for visualization\ndef denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    tensor = tensor.clone()  # Avoid modifying the original tensor\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)  # Reverse normalization: (tensor * std) + mean\n    return tensor\n\n# Get a batch of training images\nimages, labels = next(iter(train_loader))\n\n# Denormalize images for visualization\nimages = denormalize(images)\n\n# Print the number of images in the batch\nprint(f\"Number of images processed in this batch: {images.shape[0]}\")\nprint(f\"Number of images displayed: 8\")\n\n# Plot the first 8 images in the batch\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\naxes = axes.ravel()\n\nfor i in range(8):\n    img = images[i].permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n    img = np.clip(img, 0, 1)  # Ensure pixel values are in [0, 1]\n    axes[i].imshow(img)  # Since images are grayscale with 3 channels, this will show as grayscale\n    axes[i].set_title(emotion_labels[labels[i].item()])\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"3a06bd1a-2fdf-43f8-8f53-2c6959cc9e50","_cell_guid":"c010eea5-461f-4809-afa9-64e5c6789839","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-23T03:43:14.191518Z","iopub.execute_input":"2025-05-23T03:43:14.191751Z","iopub.status.idle":"2025-05-23T03:43:15.222347Z","shell.execute_reply.started":"2025-05-23T03:43:14.191733Z","shell.execute_reply":"2025-05-23T03:43:15.221484Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MLCA Module (Lightweight)\nclass MLCA(nn.Module):\n    def __init__(self, x1_dim, x2_dim, embed_dim=512, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.x1_proj = nn.Linear(x1_dim, embed_dim)\n        self.x2_proj = nn.Linear(x2_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.scale = (embed_dim // num_heads) ** -0.5\n\n    def forward(self, x1, x2):\n        x1 = self.x1_proj(x1)  # (B, N, embed_dim)\n        x2 = self.x2_proj(x2)\n        B, N, C = x1.shape\n\n        q = self.q_proj(x1).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        v = self.v_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q.reshape(B * self.num_heads, N, C // self.num_heads)\n        k = k.reshape(B * self.num_heads, N, C // self.num_heads)\n        v = v.reshape(B * self.num_heads, N, C // self.num_heads)\n\n        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.bmm(attn, v)\n\n        out = out.reshape(B, self.num_heads, N, C // self.num_heads).permute(0, 2, 1, 3).reshape(B, N, C)\n        out = self.out_proj(out)\n        return out\n\n# SwinFace Model (Lightweight) # swin_large_patch4_window7_224 # tiny, base, large\nclass SwinFace(nn.Module):\n    def __init__(self, backbone_name='swin_base_patch4_window7_224', embed_dim=512, num_heads=4, num_classes=7, max_tokens=32):\n        super().__init__()\n        self.backbone = create_model(backbone_name, pretrained=True, features_only=True)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.max_tokens = max_tokens\n\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        f1 = features[-2]  # (B, C1, H1, W1)\n        f2 = features[-1]  # (B, C2, H2, W2)\n\n        B, C1, H1, W1 = f1.shape\n        B, C2, H2, W2 = f2.shape\n\n        f1_flat = f1.flatten(2).transpose(1, 2)  # (B, H1*W1, C1)\n        f2_flat = f2.flatten(2).transpose(1, 2)  # (B, H2*W2, C2)\n\n        N = min(f1_flat.size(1), f2_flat.size(1), self.max_tokens)\n        f1_flat = f1_flat[:, :N, :]\n        f2_flat = f2_flat[:, :N, :]\n\n        if not hasattr(self, 'mlca'):\n            self.mlca = MLCA(\n                x1_dim=C1,\n                x2_dim=C2,\n                embed_dim=self.embed_dim,\n                num_heads=self.num_heads\n            ).to(x.device)\n\n        fused = self.mlca(f1_flat, f2_flat)\n        pooled = fused.mean(dim=1)\n        logits = self.classifier(pooled)\n        return logits\n\n# Device, model, loss, optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SwinFace().to(device)\n\n# Print model summary\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} Total Parameters.\")\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\nscaler = torch.amp.GradScaler()\n\n# Checkpoint logic\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nstart_epoch = 0\nbest_acc = 0.0\ncheckpoint_path = os.path.join(checkpoint_dir, \"checkpoint_swinface_model.pth\")\n\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\n# Load checkpoint if available\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint.get('epoch', 0) + 1\n    best_acc = checkpoint.get('best_acc', 0.0)\n    train_losses = checkpoint.get('train_losses', [])\n    val_losses = checkpoint.get('val_losses', [])\n    train_accs = checkpoint.get('train_accs', [])\n    val_accs = checkpoint.get('val_accs', [])\n    print(f\"Resumed training from epoch {start_epoch} with best accuracy {best_acc:.2f}%\")","metadata":{"_uuid":"7fc35ded-c60d-44bc-a847-1c7f623e6763","_cell_guid":"026b0dcc-0eba-4689-9915-437f9d42d663","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T03:43:15.223083Z","iopub.execute_input":"2025-05-23T03:43:15.223334Z","iopub.status.idle":"2025-05-23T03:43:18.656563Z","shell.execute_reply.started":"2025-05-23T03:43:15.223306Z","shell.execute_reply":"2025-05-23T03:43:18.655982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Model\npatience = 15\nepochs_no_improve = 0\nearly_stop = False\nepochs = 30\n\nbest_model_path = \"/kaggle/working/best_swinface_model.pth\"\nlast_checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_swinface_model.pth\")\n\nfor epoch in range(start_epoch, epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    train_preds, train_truths = [], []\n\n    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} [Training]\")\n    for batch_idx, (imgs, labels) in train_progress:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda'):\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n        if torch.isnan(loss):\n            print(f\"NaN loss at epoch {epoch+1}, batch {batch_idx+1}. Skipping.\")\n            continue\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n        train_preds.extend(preds.detach().cpu().numpy())\n        train_truths.extend(labels.detach().cpu().numpy())\n\n        train_progress.set_postfix({'loss': running_loss / (batch_idx + 1)})\n\n    train_loss = running_loss / len(train_loader)\n    train_acc = 100 * correct / total\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n\n    # Validation Phase\n    model.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    val_preds, val_truths = [], []\n\n    val_progress = tqdm(enumerate(test_loader), total=len(test_loader), desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False)\n    with torch.no_grad():\n        for batch_idx, (imgs, labels) in val_progress:\n            imgs, labels = imgs.to(device), labels.to(device)\n            with torch.amp.autocast('cuda'):\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n\n            val_running_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            val_total += labels.size(0)\n            val_correct += (preds == labels).sum().item()\n            val_preds.extend(preds.detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n            val_progress.set_postfix({'loss': val_running_loss / (batch_idx + 1)})\n\n    val_loss = val_running_loss / len(test_loader)\n    val_acc = 100 * val_correct / val_total\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n\n    # --- Print epoch summary ---\n    print(f\"Epoch [{epoch+1}/{epochs}]\")\n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.2f}%\")\n\n    # Save checkpoint\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'best_acc': best_acc,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n    torch.save(checkpoint, last_checkpoint_path)\n\n    # Save best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"New best model saved with Val Acc: {best_acc:.2f}%\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement in Val Acc for {epochs_no_improve} epoch(s).\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            early_stop = True\n            break\n\n    scheduler.step()","metadata":{"_uuid":"8d0ff097-5291-419d-b830-082ca64ccb65","_cell_guid":"ed2f194b-6d7d-47f1-a87b-864e30150aaa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T03:43:18.657234Z","iopub.execute_input":"2025-05-23T03:43:18.657483Z","iopub.status.idle":"2025-05-23T05:14:28.169534Z","shell.execute_reply.started":"2025-05-23T03:43:18.657464Z","shell.execute_reply":"2025-05-23T05:14:28.168456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Model\ntorch.save(model.state_dict(), '/kaggle/working/swinface_model.pth')\nprint(\"Saved final model state to /kaggle/working/swinface_model.pth\")","metadata":{"_uuid":"c81cadd9-3ec2-44e3-9d70-4b6697b08542","_cell_guid":"0a175e0a-2871-4bbe-a2be-492c39f13b77","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T05:14:28.170695Z","iopub.execute_input":"2025-05-23T05:14:28.171002Z","iopub.status.idle":"2025-05-23T05:14:28.634745Z","shell.execute_reply.started":"2025-05-23T05:14:28.170969Z","shell.execute_reply":"2025-05-23T05:14:28.634079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate model\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    roc_curve,\n    auc\n)\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load Best Model\nbest_model_path = \"/kaggle/working/best_swinface_model.pth\"\nlast_checkpoint_path = \"/kaggle/working/checkpoints/checkpoint_swinface_model.pth\"\n\nif os.path.exists(best_model_path):\n    model.load_state_dict(torch.load(best_model_path, map_location=device))\n    print(\"Loaded best model for evaluation.\")\nelif os.path.exists(last_checkpoint_path):\n    checkpoint = torch.load(last_checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(\"Loaded last checkpoint model (best model not found).\")\nelse:\n    print(\"No model found.\")\n    test_acc = 0.0\n\n# Inference Loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nall_preds = []\nall_labels = []\nall_probs = []\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n\n        _, predicted = torch.max(outputs, 1)\n\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n# Basic Accuracy\ntest_acc = 100 * correct / total if total > 0 else 0.0\nprint(f\"\\nTest Accuracy: {test_acc:.2f}%\")\n\n# Convert to numpy arrays\ny_true = np.array(all_labels)\ny_pred = np.array(all_preds)\ny_prob = np.array(all_probs)\ny_true_bin = label_binarize(y_true, classes=range(7))  # 7 classes\n\n# Metrics\nprecision = precision_score(y_true, y_pred, average='macro') * 100\nrecall = recall_score(y_true, y_pred, average='macro') * 100\nf1 = f1_score(y_true, y_pred, average='macro') * 100\nroc_auc = roc_auc_score(y_true_bin, y_prob, average='macro', multi_class='ovr')\n\nprint(f\"\\nPrecision (macro): {precision:.2f}%\")\nprint(f\"Recall (macro): {recall:.2f}%\")\nprint(f\"F1-score (macro): {f1:.2f}%\")\nprint(f\"ROC-AUC (macro): {roc_auc:.2f}\")\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=list(emotion_labels.values())))\n\n# Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_labels.values(), yticklabels=emotion_labels.values())\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.tight_layout()\nplt.show()\n\n# ROC Curve (One-vs-Rest)\nfpr = dict()\ntpr = dict()\nroc_auc_per_class = dict()\nfor i in range(7):\n    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n    roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n\n# Plot ROC curves\nplt.figure(figsize=(10, 8))\nfor i in range(7):\n    plt.plot(fpr[i], tpr[i], label=f'{emotion_labels[i]} (AUC = {roc_auc_per_class[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.title(\"ROC Curves per Class (One-vs-Rest)\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n# Step: Visualize plot training curve\nif train_losses and val_losses and train_accs and val_accs:\n    plt.figure(figsize=(14, 6))\n\n    # Loss Curve\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label=\"Train Loss\", color='blue', linewidth=2)\n    plt.plot(val_losses, label=\"Validation Loss\", color='orange', linewidth=2)\n    plt.title(\"Loss over Epochs\", fontsize=14)\n    plt.xlabel(\"Epoch\", fontsize=12)\n    plt.ylabel(\"Loss\", fontsize=12)\n    plt.legend()\n    plt.grid(True)\n\n    # Accuracy Curve\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label=\"Train Accuracy\", color='green', linewidth=2)\n    plt.plot(val_accs, label=\"Validation Accuracy\", color='red', linewidth=2)\n    plt.title(\"Accuracy over Epochs\", fontsize=14)\n    plt.xlabel(\"Epoch\", fontsize=12)\n    plt.ylabel(\"Accuracy (%)\", fontsize=12)\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/training_curves.png\")\n    plt.show()\n    print(\"Saved training curves to `/kaggle/working/training_curves.png`\")\nelse:\n    print(\"Training curves not plotted. Lists are empty (training may not have completed).\")","metadata":{"_uuid":"4a5709b4-bc58-487c-a85c-76c89d4029d4","_cell_guid":"dcf76d00-1821-4ba9-a27f-8f7b4f3f1f3d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T05:14:28.635499Z","iopub.execute_input":"2025-05-23T05:14:28.635685Z","iopub.status.idle":"2025-05-23T05:15:15.856935Z","shell.execute_reply.started":"2025-05-23T05:14:28.635670Z","shell.execute_reply":"2025-05-23T05:15:15.856220Z"}},"outputs":[],"execution_count":null}]}