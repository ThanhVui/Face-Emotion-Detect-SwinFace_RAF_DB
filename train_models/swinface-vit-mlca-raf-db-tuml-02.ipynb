{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom timm import create_model\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom tqdm import tqdm","metadata":{"_uuid":"7c120d33-6a80-40df-9a49-99e2ca972659","_cell_guid":"140f51ee-94e8-4f30-b8a9-cc489adb6627","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:41:33.292101Z","iopub.execute_input":"2025-05-19T07:41:33.292355Z","iopub.status.idle":"2025-05-19T07:41:45.395608Z","shell.execute_reply.started":"2025-05-19T07:41:33.292331Z","shell.execute_reply":"2025-05-19T07:41:45.395025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset class for RAF-DB\nclass RAFDataset(Dataset):\n    EMOTIONS = {1: 1, 2: 5, 3: 4, 4: 0, 5: 2, 6: 3, 7: 6}  # RAF-DB labels to 0-6\n    \n    def __init__(self, root_dir, split_file, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        with open(split_file, 'r') as f:\n            lines = f.readlines()[1:]  # Skip header\n        for line in lines:\n            parts = line.strip().split(',')\n            if len(parts) != 2:\n                print(f\"Skipping malformed line: {line.strip()}\")\n                continue\n            img_name, label = parts\n            try:\n                label_int = int(label)\n                expected_subfolder = str(label_int)\n                base_img_name = img_name\n                img_path = os.path.join(root_dir, expected_subfolder, base_img_name)\n                found = False\n                if os.path.exists(img_path):\n                    found = True\n                else:\n                    if '_aligned' in base_img_name:\n                        alt_img_name = base_img_name.replace('_aligned', '')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                    elif not base_img_name.endswith('_aligned.jpg'):\n                        alt_img_name = base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, expected_subfolder, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            base_img_name = alt_img_name\n                if not found:\n                    for subfolder in range(1, 8):\n                        subfolder_str = str(subfolder)\n                        img_path = os.path.join(root_dir, subfolder_str, base_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {base_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            break\n                        alt_img_name = base_img_name.replace('_aligned', '') if '_aligned' in base_img_name else base_img_name.replace('.jpg', '_aligned.jpg')\n                        img_path = os.path.join(root_dir, subfolder_str, alt_img_name)\n                        if os.path.exists(img_path):\n                            found = True\n                            print(f\"Found {alt_img_name} in {root_dir}/{subfolder_str}/ (expected {root_dir}/{expected_subfolder}/)\")\n                            base_img_name = alt_img_name\n                            break\n                if not found:\n                    print(f\"Warning: Image not found after searching all subfolders: {base_img_name}\")\n                    continue\n                mapped_label = self.EMOTIONS[label_int]\n                self.images.append(img_path)\n                self.labels.append(mapped_label)\n            except (ValueError, KeyError):\n                print(f\"Skipping invalid label in line: {line.strip()}\")\n                continue\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.images[idx]).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"_uuid":"86550d5c-e198-4719-b2b6-79fa712d1e67","_cell_guid":"3f1a11e3-f300-4f0a-8f72-449a54786abd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:41:45.397046Z","iopub.execute_input":"2025-05-19T07:41:45.397446Z","iopub.status.idle":"2025-05-19T07:41:45.407876Z","shell.execute_reply.started":"2025-05-19T07:41:45.397426Z","shell.execute_reply":"2025-05-19T07:41:45.407067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MLCA Module (Lightweight)\nclass MLCA(nn.Module):\n    def __init__(self, x1_dim, x2_dim, embed_dim=512, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.x1_proj = nn.Linear(x1_dim, embed_dim)\n        self.x2_proj = nn.Linear(x2_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.scale = (embed_dim // num_heads) ** -0.5\n\n    def forward(self, x1, x2):\n        x1 = self.x1_proj(x1)  # (B, N, embed_dim)\n        x2 = self.x2_proj(x2)\n        B, N, C = x1.shape\n\n        q = self.q_proj(x1).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        v = self.v_proj(x2).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q.reshape(B * self.num_heads, N, C // self.num_heads)\n        k = k.reshape(B * self.num_heads, N, C // self.num_heads)\n        v = v.reshape(B * self.num_heads, N, C // self.num_heads)\n\n        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.bmm(attn, v)\n\n        out = out.reshape(B, self.num_heads, N, C // self.num_heads).permute(0, 2, 1, 3).reshape(B, N, C)\n        out = self.out_proj(out)\n        return out\n\n\n\n# SwinFace Model (Lightweight)\nclass SwinFace(nn.Module):\n    def __init__(self, backbone_name='swin_base_patch4_window7_224', embed_dim=512, num_heads=4, num_classes=7, max_tokens=32):\n        super().__init__()\n        self.backbone = create_model(backbone_name, pretrained=True, features_only=True)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.max_tokens = max_tokens\n\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        f1 = features[-2]  # (B, C1, H1, W1)\n        f2 = features[-1]  # (B, C2, H2, W2)\n\n        B, C1, H1, W1 = f1.shape\n        B, C2, H2, W2 = f2.shape\n\n        f1_flat = f1.flatten(2).transpose(1, 2)  # (B, H1*W1, C1)\n        f2_flat = f2.flatten(2).transpose(1, 2)  # (B, H2*W2, C2)\n\n        N = min(f1_flat.size(1), f2_flat.size(1), self.max_tokens)\n        f1_flat = f1_flat[:, :N, :]\n        f2_flat = f2_flat[:, :N, :]\n\n        if not hasattr(self, 'mlca'):\n            self.mlca = MLCA(\n                x1_dim=C1,\n                x2_dim=C2,\n                embed_dim=self.embed_dim,\n                num_heads=self.num_heads\n            ).to(x.device)\n\n        fused = self.mlca(f1_flat, f2_flat)\n        pooled = fused.mean(dim=1)\n        logits = self.classifier(pooled)\n        return logits","metadata":{"_uuid":"469b60a8-cc21-4bb3-b36b-8513f012ac51","_cell_guid":"ca01a007-07fe-480a-964e-c3d497fc710e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:41:45.410680Z","iopub.execute_input":"2025-05-19T07:41:45.410910Z","iopub.status.idle":"2025-05-19T07:41:45.456799Z","shell.execute_reply.started":"2025-05-19T07:41:45.410891Z","shell.execute_reply":"2025-05-19T07:41:45.456106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"_uuid":"961a2ca8-f3bc-4f2c-806b-b25b4e305ee2","_cell_guid":"af676e58-372a-4a12-97fc-1d40392e7545","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:41:45.457479Z","iopub.execute_input":"2025-05-19T07:41:45.457668Z","iopub.status.idle":"2025-05-19T07:41:45.474209Z","shell.execute_reply.started":"2025-05-19T07:41:45.457653Z","shell.execute_reply":"2025-05-19T07:41:45.473787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Loaders\ntrain_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/train',\n    split_file='/kaggle/input/train_labels.csv',\n    transform=train_transform\n)\ntest_dataset = RAFDataset(\n    root_dir='/kaggle/input/DATASET/test',\n    split_file='/kaggle/input/test_labels.csv',\n    transform=test_transform\n)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of test samples: {len(test_dataset)}\")\n\nif len(train_dataset) == 0 or len(test_dataset) == 0:\n    raise ValueError(\"One or both datasets are empty. Check the warnings above for missing images.\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)  # Reduced batch size to 16\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)  # Reduced batch size to 16","metadata":{"_uuid":"f8015c47-189d-4dc6-a6f9-07a071e3bd6b","_cell_guid":"929b6cbc-c6c8-4a62-9cd4-33239abda3cb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:41:45.474607Z","iopub.execute_input":"2025-05-19T07:41:45.474757Z","iopub.status.idle":"2025-05-19T07:42:09.933528Z","shell.execute_reply.started":"2025-05-19T07:41:45.474743Z","shell.execute_reply":"2025-05-19T07:42:09.932784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.amp import GradScaler\n\n# Model Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SwinFace().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\nscaler = torch.amp.GradScaler()","metadata":{"_uuid":"a29e107a-2b97-4e1f-b120-5c2c41e101b0","_cell_guid":"86f5b900-dddc-47e7-bbac-f8897082ed25","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T07:42:09.935135Z","iopub.execute_input":"2025-05-19T07:42:09.935389Z","iopub.status.idle":"2025-05-19T07:42:13.406097Z","shell.execute_reply.started":"2025-05-19T07:42:09.935372Z","shell.execute_reply":"2025-05-19T07:42:13.405506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport os\n\n# Checkpoint Management Setup\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\ncheckpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n\nepochs = 15\ntrain_losses = []\nval_losses = []\ntrain_accs = []\nval_accs = []\n\n# Early stopping variables\npatience = 5\nepochs_no_improve = 0\nbest_acc = 0.0\nstart_epoch = 0\nearly_stop = False\n\n# Load checkpoint if it exists\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    best_acc = checkpoint['best_acc']\n    train_losses = checkpoint.get('train_losses', [])\n    val_losses = checkpoint.get('val_losses', [])\n    train_accs = checkpoint.get('train_accs', [])\n    val_accs = checkpoint.get('val_accs', [])\n    print(f\"Resumed training from epoch {start_epoch} with best accuracy {best_acc*100:.2f}%\")\n\nfor epoch in range(start_epoch, epochs):\n    model.train()\n    epoch_train_losses = []\n    train_preds, train_truths = [], []\n\n    for batch_idx, (imgs, labels) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda'):\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n        if torch.isnan(loss):\n            print(f\"NaN loss at epoch {epoch+1}, batch {batch_idx+1}. Skipping.\")\n            continue\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        epoch_train_losses.append(loss.item())\n        train_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n        train_truths.extend(labels.detach().cpu().numpy())\n\n    avg_train_loss = np.mean(epoch_train_losses)\n    train_acc = accuracy_score(train_truths, train_preds)\n    train_losses.append(avg_train_loss)\n    train_accs.append(train_acc)\n\n    model.eval()\n    epoch_val_losses = []\n    val_preds, val_truths = [], []\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validating\"):\n            imgs, labels = imgs.to(device), labels.to(device)\n            with torch.amp.autocast('cuda'):\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n                epoch_val_losses.append(loss.item())\n\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n    avg_val_loss = np.mean(epoch_val_losses)\n    val_acc = accuracy_score(val_truths, val_preds)\n    val_losses.append(avg_val_loss)\n    val_accs.append(val_acc)\n\n    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n    scheduler.step()\n\n    # Save checkpoint\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'best_acc': best_acc,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_accs': train_accs,\n        'val_accs': val_accs\n    }\n    torch.save(checkpoint, checkpoint_path)\n\n    # Update best model and early stopping\n    if val_acc > best_acc:\n        best_acc = val_acc\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), \"/kaggle/working/best_model.pth\")\n        print(f\"Saved best model with Val Acc: {best_acc*100:.2f}%\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement in Val Acc for {epochs_no_improve} epochs.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            early_stop = True\n            break\n\n    if early_stop:\n        break","metadata":{"_uuid":"3a40d001-1eee-49c3-94e3-d30064520aa3","_cell_guid":"473d57ce-c014-4503-b736-3da39d9fe06d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T08:44:54.731119Z","iopub.execute_input":"2025-05-19T08:44:54.731517Z","iopub.status.idle":"2025-05-19T08:53:19.265485Z","shell.execute_reply.started":"2025-05-19T08:44:54.731493Z","shell.execute_reply":"2025-05-19T08:53:19.264563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Model\ntorch.save(model.state_dict(), '/kaggle/working/swinface_model.pth')","metadata":{"_uuid":"9c5fd56d-144b-48b9-b126-302c497a2699","_cell_guid":"a7aae05f-166d-40c4-bd1d-0d848daadb2f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-19T08:39:59.684781Z","iopub.execute_input":"2025-05-19T08:39:59.685039Z","iopub.status.idle":"2025-05-19T08:40:00.126015Z","shell.execute_reply.started":"2025-05-19T08:39:59.685013Z","shell.execute_reply":"2025-05-19T08:40:00.125455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training Curves\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(epoch_train_losses)+1), epoch_train_losses, label='Train Loss')\nplt.plot(range(1, len(epoch_val_losses)+1), epoch_val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Acc')\nplt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"_uuid":"2c230b4b-4800-4a2a-b7a3-7a91561a4556","_cell_guid":"e4c86079-5272-476d-baa6-41560b3d782f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-19T08:40:00.126896Z","iopub.execute_input":"2025-05-19T08:40:00.127089Z","iopub.status.idle":"2025-05-19T08:40:00.406740Z","shell.execute_reply.started":"2025-05-19T08:40:00.127074Z","shell.execute_reply":"2025-05-19T08:40:00.406018Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_swinface_model.pth'))\n\n# Evaluate best model\nmodel.eval()\nval_preds = []\nval_truths = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, desc=\"Evaluating best model\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        with torch.amp.autocast('cuda'):\n            outputs = model(imgs)\n            val_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n            val_truths.extend(labels.detach().cpu().numpy())\n\n# Confusion Matrix\ncm = confusion_matrix(val_truths, val_preds)\nemotion_labels = ['Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Neutral']\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion_labels, yticklabels=emotion_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"_uuid":"a59969f6-0297-44c5-b8b2-14d8bfba73cc","_cell_guid":"905962d0-b4f9-46a3-bf78-0aa50320ad72","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-19T08:40:00.407557Z","iopub.execute_input":"2025-05-19T08:40:00.407820Z","iopub.status.idle":"2025-05-19T08:40:19.012011Z","shell.execute_reply.started":"2025-05-19T08:40:00.407798Z","shell.execute_reply":"2025-05-19T08:40:19.011234Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate Model\nmodel.eval()\ncorrect = 0\ntotal = 0\nall_preds = []\nall_labels = []\n\n# Load the best model for evaluation\nif os.path.exists(\"/kaggle/working/best_model.pth\"):\n    model.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\", map_location=device))\n    print(\"Loaded best model for evaluation.\")\nelse:\n    print(\"Best model not found. Evaluating with the final model.\")\n    if os.path.exists(\"/kaggle/working/swinface_model.pth\"):\n        model.load_state_dict(torch.load(\"/kaggle/working/swinface_model.pth\", map_location=device))\n    else:\n        print(\"No saved model found for evaluation.\")\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n        images, labels = images.to(device), labels.to(device)\n        with torch.amp.autocast('cuda'):\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\nif total > 0:\n    test_acc = 100 * correct / total\n    print(f\"Test Accuracy: {test_acc:.2f}%\")\nelse:\n    print(\"Test set is empty or not loaded correctly. Cannot compute accuracy.\")\n    test_acc = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict Emotion Function\ndef predict_emotion(img_path, model, transform, device):\n    img = Image.open(img_path).convert('RGB')\n    img = transform(img).unsqueeze(0).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        with torch.amp.autocast('cuda'):\n            output = model(img)\n            pred = output.argmax(dim=1).item()\n    \n    emotion_labels = ['Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Neutral']\n    return emotion_labels[pred]\n\n# Example usage\nimg_path = '/kaggle/input/raf-db-dataset/DATASET/test/4/test_0003_aligned.jpg'\npredicted_emotion = predict_emotion(img_path, model, test_transform, device)\nprint(f'Predicted emotion: {predicted_emotion}')","metadata":{"_uuid":"af2256a5-ce42-4ea4-a750-bdb14df90c8e","_cell_guid":"99660d90-e4cf-4214-930e-ea8cdebadd17","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-19T08:40:19.012811Z","iopub.execute_input":"2025-05-19T08:40:19.012998Z","iopub.status.idle":"2025-05-19T08:40:19.074300Z","shell.execute_reply.started":"2025-05-19T08:40:19.012980Z","shell.execute_reply":"2025-05-19T08:40:19.073519Z"},"_kg_hide-input":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}